{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3044e07",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook A — MNIST Stacked Ensemble (VGG16 · VGG19 · ResNet50 + Meta LR)\n",
    "\n",
    "**Paper Companion:** *Optimizing Handwritten Digit Recognition: A Novel Stacked CNN Ensemble for Enhanced Feature Extraction*  \n",
    "\n",
    "**What this notebook does**  \n",
    "- Small, **balanced subset** for speed (adjustable)  \n",
    "- **Transfer learning** with explicit preprocess + **penultimate stacking**  \n",
    "- **50 epochs** per base model (as requested, for consistent learning curves)  \n",
    "- **k-fold CV** with multiple seeds + **95% CI**  \n",
    "- **Significance tests** (Wilcoxon, McNemar)  \n",
    "- **Figures** exported for the paper (training curves, feature maps, confusion matrix, CV plots)  \n",
    "- **Colab-ready** (includes a quick `%pip install` cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c64b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# 0) Setup (Colab / first run)\n",
    "# ===========================\n",
    "# If running on Colab (or first time locally), uncomment this cell:\n",
    "# %pip install -q tensorflow tensorflow-datasets scikit-learn matplotlib pandas numpy scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# Notebook A: MNIST Stacked Ensemble (VGG16/VGG19/ResNet50 + Meta LR)\n",
    "# - Small balanced subset for speed (adjustable)\n",
    "# - Transfer learning + penultimate stacking\n",
    "# - k-fold CV with multiple seeds + 95% CI\n",
    "# - Significance tests (Wilcoxon, McNemar)\n",
    "# - Plots & exports for paper\n",
    "# Colab-ready, TF 2.x\n",
    "# ================================================================\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Imports & global config\n",
    "# ---------------------------\n",
    "import os, time, math, json, numpy as np, tensorflow as tf, matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG16, VGG19, ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# Output folders\n",
    "OUT = Path(\"paper_outputs_mnist\")\n",
    "OUT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Hyperparameters & logging\n",
    "# ---------------------------\n",
    "HP = {\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"num_classes\": 10,\n",
    "    \"img_size\": 224,\n",
    "    \"batch_size\": 8,\n",
    "    \"train_subset_n\": 500,    # balanced total; increase for final runs\n",
    "    \"test_subset_n\": 100,     # balanced total\n",
    "    \"epochs_per_model\": 50,   # <-- set to 50 to reproduce long learning curves\n",
    "    \"fine_tune_last_layers\": {\"vgg16\": 4, \"vgg19\": 4, \"resnet50\": 10},\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"augment\": {\n",
    "        \"random_flip\": \"HORIZONTAL\",   # none / horizontal / vertical\n",
    "        \"random_rotation\": 0.08,        # ~4.5 degrees\n",
    "        \"random_zoom\": 0.08,\n",
    "    },\n",
    "    \"cv_kfolds\": 3,            # use 5 for final paper\n",
    "    \"cv_seeds\": [42, 43],      # use 5 seeds for final paper: [42,43,44,45,46]\n",
    "}\n",
    "with open(OUT / \"hyperparams.json\", \"w\") as f:\n",
    "    json.dump(HP, f, indent=2)\n",
    "\n",
    "# Device info\n",
    "dev_info = {\n",
    "    \"tf_version\": tf.__version__,\n",
    "    \"gpus\": [x.name for x in tf.config.list_physical_devices('GPU')],\n",
    "    \"cpus\": [x.name for x in tf.config.list_physical_devices('CPU')],\n",
    "}\n",
    "with open(OUT / \"device_info.json\", \"w\") as f:\n",
    "    json.dump(dev_info, f, indent=2)\n",
    "print(\"Device info:\", dev_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74086cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------\n",
    "# 3) Load MNIST & build small balanced subsets\n",
    "# ---------------------------------------------\n",
    "(x_train_full, y_train_full), (x_test_full, y_test_full) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def make_balanced_subset(x, y, total_count, num_classes=10, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    per_class = max(1, total_count // num_classes)\n",
    "    xs, ys = [], []\n",
    "    for c in range(num_classes):\n",
    "        idx = np.where(y == c)[0]\n",
    "        pick = rng.choice(idx, size=per_class, replace=False)\n",
    "        xs.append(x[pick]); ys.append(y[pick])\n",
    "    return np.concatenate(xs), np.concatenate(ys)\n",
    "\n",
    "x_train, y_train = make_balanced_subset(x_train_full, y_train_full, HP[\"train_subset_n\"])\n",
    "x_test,  y_test  = make_balanced_subset(x_test_full,  y_test_full,  HP[\"test_subset_n\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------\n",
    "# 4) Preprocess & augmentation (explicit params)\n",
    "# ---------------------------------------------\n",
    "IMG_SIZE = HP[\"img_size\"]\n",
    "NUM_CLASSES = HP[\"num_classes\"]\n",
    "BATCH_SIZE = HP[\"batch_size\"]\n",
    "\n",
    "# augmentation layer (for training only)\n",
    "data_augment = tf.keras.Sequential(name=\"augment\")\n",
    "if HP[\"augment\"][\"random_flip\"].upper() == \"HORIZONTAL\":\n",
    "    data_augment.add(layers.RandomFlip(\"horizontal\"))\n",
    "elif HP[\"augment\"][\"random_flip\"].upper() == \"VERTICAL\":\n",
    "    data_augment.add(layers.RandomFlip(\"vertical\"))\n",
    "# rotations & zooms:\n",
    "data_augment.add(layers.RandomRotation(HP[\"augment\"][\"random_rotation\"]))\n",
    "data_augment.add(layers.RandomZoom(HP[\"augment\"][\"random_zoom\"]))\n",
    "\n",
    "def preprocess(image, label, training=False):\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    if training:\n",
    "        image = data_augment(image)\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "SHUF = min(len(x_train), 1000)\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "            .shuffle(SHUF, seed=SEED)\n",
    "            .map(lambda i,l: preprocess(i,l, True), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds = (tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "          .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .batch(BATCH_SIZE)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "for imgs, labs in train_ds.take(1):\n",
    "    print(\"Train batch:\", imgs.shape, labs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea318d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class distribution plots (for paper)\n",
    "def plot_class_distribution(labels, title, save_path):\n",
    "    counts = np.bincount(labels, minlength=NUM_CLASSES)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar(np.arange(NUM_CLASSES), counts)\n",
    "    plt.title(title); plt.xlabel(\"Digit\"); plt.ylabel(\"Count\")\n",
    "    plt.xticks(np.arange(NUM_CLASSES)); plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(save_path, dpi=300); plt.show()\n",
    "\n",
    "plot_class_distribution(y_train, \"Training Set Class Distribution (subset)\", OUT/\"class_dist_train.png\")\n",
    "plot_class_distribution(y_test,  \"Testing Set Class Distribution (subset)\",  OUT/\"class_dist_test.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) Build base models (VGG16, VGG19, ResNet50)\n",
    "#    - include explicit preprocess_input\n",
    "#    - unfreeze top-k layers for light fine-tuning\n",
    "# -------------------------------------------------------\n",
    "def build_base_model(base_cls, fine_tune_at, lr=HP[\"learning_rate\"], name_tag=\"\"):\n",
    "    # choose preprocess according to backbone\n",
    "    if base_cls in [VGG16, VGG19]:\n",
    "        preprocess_fn = tf.keras.applications.vgg16.preprocess_input\n",
    "    elif base_cls is ResNet50:\n",
    "        preprocess_fn = tf.keras.applications.resnet50.preprocess_input\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "    inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='input_rgb_0to1')\n",
    "    x   = layers.Lambda(preprocess_fn, name='imagenet_preprocess')(inp)\n",
    "    base = base_cls(weights='imagenet', include_top=False, input_tensor=x)\n",
    "\n",
    "    for lay in base.layers: lay.trainable = False\n",
    "    for lay in base.layers[-fine_tune_at:]: lay.trainable = True\n",
    "\n",
    "    # head\n",
    "    y = layers.GlobalAveragePooling2D(name='gap')(base.output)\n",
    "    y = layers.Dropout(0.5, name='drop1')(y)\n",
    "    y = layers.Dense(256, activation='relu', name='fc1')(y)\n",
    "    y = layers.Dropout(0.5, name='drop2')(y)\n",
    "    out= layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(y)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out, name=f\"{base_cls.__name__}_HDR{name_tag}\")\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "vgg16_model    = build_base_model(VGG16,  HP[\"fine_tune_last_layers\"][\"vgg16\"])\n",
    "vgg19_model    = build_base_model(VGG19,  HP[\"fine_tune_last_layers\"][\"vgg19\"])\n",
    "resnet50_model = build_base_model(ResNet50,HP[\"fine_tune_last_layers\"][\"resnet50\"])\n",
    "\n",
    "# Parameter counts (for computational reporting)\n",
    "comp_report = {}\n",
    "for name, m in [(\"vgg16\", vgg16_model), (\"vgg19\", vgg19_model), (\"resnet50\", resnet50_model)]:\n",
    "    comp_report[name] = {\"trainable_params\": int(np.sum([np.prod(v.shape) for v in m.trainable_weights])),\n",
    "                         \"non_trainable_params\": int(np.sum([np.prod(v.shape) for v in m.non_trainable_weights]))}\n",
    "with open(OUT/\"param_counts.json\", \"w\") as f: json.dump(comp_report, f, indent=2)\n",
    "print(\"Param counts:\", comp_report)\n",
    "\n",
    "# Optional FLOPs estimation (may fail depending on TF build; ignored if it does)\n",
    "def try_estimate_flops(model):\n",
    "    try:\n",
    "        concrete = tf.function(model).get_concrete_function(tf.TensorSpec([1, IMG_SIZE, IMG_SIZE, 3], tf.float32))\n",
    "        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "        frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete)\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "        return int(flops.total_float_ops)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "for name, m in [(\"vgg16\", vgg16_model), (\"vgg19\", vgg19_model), (\"resnet50\", resnet50_model)]:\n",
    "    fl = try_estimate_flops(m)\n",
    "    comp_report[name][\"approx_FLOPs\"] = fl\n",
    "with open(OUT/\"param_counts.json\", \"w\") as f: json.dump(comp_report, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Train base models (50 epochs) & save learning curves\n",
    "# -------------------------------------------------------\n",
    "def train_and_time(model, train_ds, val_ds, epochs, tag):\n",
    "    t0 = time.time()\n",
    "    hist = model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=2)\n",
    "    dt = time.time() - t0\n",
    "    # save history CSV\n",
    "    pd.DataFrame(hist.history).to_csv(OUT/f\"history_{tag}.csv\", index=False)\n",
    "    return hist, dt\n",
    "\n",
    "histories = {}\n",
    "train_times = {}\n",
    "for name, model in [(\"vgg16\", vgg16_model), (\"vgg19\", vgg19_model), (\"resnet50\", resnet50_model)]:\n",
    "    print(f\"\\nTraining {name} ...\")\n",
    "    h, dt = train_and_time(model, train_ds, val_ds, HP[\"epochs_per_model\"], name)\n",
    "    histories[name] = h\n",
    "    train_times[name] = dt\n",
    "with open(OUT/\"train_times_seconds.json\",\"w\") as f: json.dump(train_times, f, indent=2)\n",
    "print(\"Train times (sec):\", train_times)\n",
    "\n",
    "# 3×2 training curves figure\n",
    "def _plot_pair(ax_acc, ax_loss, history, title_prefix):\n",
    "    ep = range(1, len(history.history['accuracy'])+1)\n",
    "    ax_acc.plot(ep, history.history['accuracy'], marker='o', ms=3, lw=1.5, label='Training Acc')\n",
    "    ax_acc.plot(ep, history.history['val_accuracy'], marker='*', ms=3, lw=1.5, ls='--', label='Validation Acc')\n",
    "    ax_acc.set_title(f'Training and Validation Accuracy for {title_prefix} Model', fontsize=9)\n",
    "    ax_acc.set_xlabel('Epochs', fontsize=8); ax_acc.set_ylabel('Accuracy', fontsize=8); ax_acc.grid(True, alpha=0.25); ax_acc.legend(loc='lower right', fontsize=7)\n",
    "\n",
    "    ax_loss.plot(ep, history.history['loss'], marker='o', ms=3, lw=1.5, label='Training Loss')\n",
    "    ax_loss.plot(ep, history.history['val_loss'], marker='*', ms=3, lw=1.5, ls='--', label='Validation Loss')\n",
    "    ax_loss.set_title('Training and Validation Loss', fontsize=9)\n",
    "    ax_loss.set_xlabel('Epochs', fontsize=8); ax_loss.set_ylabel('Loss', fontsize=8); ax_loss.grid(True, alpha=0.25); ax_loss.legend(loc='upper right', fontsize=7)\n",
    "\n",
    "def plot_3x2(histories, save_path):\n",
    "    fig, axes = plt.subplots(3,2, figsize=(8.5,11))\n",
    "    _plot_pair(axes[0,0], axes[0,1], histories[\"vgg16\"], \"VGG16\")\n",
    "    _plot_pair(axes[1,0], axes[1,1], histories[\"vgg19\"], \"VGG19\")\n",
    "    _plot_pair(axes[2,0], axes[2,1], histories[\"resnet50\"], \"ResNet50\")\n",
    "    for r in axes:\n",
    "        for a in r: a.tick_params(labelsize=8)\n",
    "    plt.tight_layout(pad=1.2); plt.savefig(save_path, dpi=300, bbox_inches='tight'); plt.show()\n",
    "\n",
    "plot_3x2(histories, OUT/\"training_curves_3x2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cff037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) Feature maps: first 8 maps from VGG16 block1_conv1\n",
    "# -------------------------------------------------------\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "sample = x_train[0]\n",
    "img = tf.image.resize(tf.expand_dims(sample, -1), [IMG_SIZE, IMG_SIZE])\n",
    "img = tf.image.grayscale_to_rgb(img)\n",
    "img_b = tf.expand_dims(tf.cast(img, tf.float32), 0) / 255.0\n",
    "img_b_pp = preprocess_input(img_b)  # VGG16 preprocess\n",
    "\n",
    "layer_model = tf.keras.Model(inputs=vgg16_model.input,\n",
    "                             outputs=vgg16_model.get_layer('block1_conv1').output)\n",
    "fmaps = layer_model.predict(img_b_pp, verbose=0)\n",
    "\n",
    "fig, axes = plt.subplots(1,8, figsize=(20,5))\n",
    "for i in range(8):\n",
    "    axes[i].imshow(fmaps[0,:,:,i], cmap='viridis'); axes[i].axis('off')\n",
    "plt.suptitle(\"First 8 Feature Maps from block1_conv1 (VGG16)\", fontsize=14)\n",
    "plt.tight_layout(); plt.savefig(OUT/\"feature_maps_block1conv1_vgg16.png\", dpi=300); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) Evaluate base models; build penultimate features + Meta-learner\n",
    "# -------------------------------------------------------\n",
    "def to_rgb_array(x_uint8):\n",
    "    x = tf.convert_to_tensor(x_uint8)\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    x = tf.image.resize(x, [IMG_SIZE, IMG_SIZE])\n",
    "    x = tf.image.grayscale_to_rgb(x)\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    return x.numpy()\n",
    "\n",
    "x_test_rgb = to_rgb_array(x_test)\n",
    "y_test_1h = tf.one_hot(y_test, NUM_CLASSES)\n",
    "\n",
    "acc_vgg16   = vgg16_model.evaluate(x_test_rgb, y_test_1h, verbose=0)[1]\n",
    "acc_vgg19   = vgg19_model.evaluate(x_test_rgb, y_test_1h, verbose=0)[1]\n",
    "acc_resnet  = resnet50_model.evaluate(x_test_rgb, y_test_1h, verbose=0)[1]\n",
    "print(f\"Test Acc — VGG16: {acc_vgg16:.4f}  VGG19: {acc_vgg19:.4f}  ResNet50: {acc_resnet:.4f}\")\n",
    "\n",
    "def build_penultimate(model, layer_name='gap'):\n",
    "    return tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "feat_vgg16   = build_penultimate(vgg16_model,  'gap')\n",
    "feat_vgg19   = build_penultimate(vgg19_model,  'gap')\n",
    "feat_resnet  = build_penultimate(resnet50_model,'gap')\n",
    "\n",
    "def predict_feats(fm, x, bs=64):\n",
    "    arr = []\n",
    "    for i in range(0, len(x), bs):\n",
    "        arr.append(fm.predict(x[i:i+bs], verbose=0))\n",
    "    return np.vstack(arr)\n",
    "\n",
    "x_train_rgb = to_rgb_array(x_train)\n",
    "F_tr = [predict_feats(fm, x_train_rgb) for fm in [feat_vgg16, feat_vgg19, feat_resnet]]\n",
    "F_te = [predict_feats(fm, x_test_rgb)  for fm in [feat_vgg16, feat_vgg19, feat_resnet]]\n",
    "X_tr = np.concatenate(F_tr, axis=1)\n",
    "X_te = np.concatenate(F_te, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_s = scaler.fit_transform(X_tr)\n",
    "X_te_s = scaler.transform(X_te)\n",
    "\n",
    "meta = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n",
    "meta.fit(X_tr_s, y_train)\n",
    "y_pred_meta = meta.predict(X_te_s)\n",
    "meta_acc = accuracy_score(y_test, y_pred_meta)\n",
    "print(f\"Meta-learner (hold-out test) accuracy: {meta_acc:.4f}\")\n",
    "\n",
    "# Meta-learner accuracy over iterations (SGD)\n",
    "classes = np.unique(y_train)\n",
    "sgd = SGDClassifier(loss=\"log_loss\", max_iter=1, learning_rate=\"optimal\", tol=None, random_state=SEED, warm_start=True)\n",
    "acc_iters = []\n",
    "EPOCHS_SGD = 30\n",
    "sgd.partial_fit(X_tr_s, y_train, classes=classes)\n",
    "acc_iters.append(accuracy_score(y_test, sgd.predict(X_te_s)))\n",
    "for _ in range(EPOCHS_SGD-1):\n",
    "    sgd.partial_fit(X_tr_s, y_train)\n",
    "    acc_iters.append(accuracy_score(y_test, sgd.predict(X_te_s)))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(range(1, EPOCHS_SGD+1), acc_iters, marker='o', lw=2)\n",
    "plt.title(\"Meta-Learner Accuracy Over Iterations\"); plt.xlabel(\"Iteration\"); plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.savefig(OUT/\"meta_accuracy_over_iterations.png\", dpi=300); plt.show()\n",
    "\n",
    "# Comparison bar chart\n",
    "labels = [\"VGG16\",\"VGG19\",\"ResNet50\",\"Meta-LR\"]\n",
    "scores = [acc_vgg16, acc_vgg19, acc_resnet, meta_acc]\n",
    "plt.figure(figsize=(6.5,4))\n",
    "bars = plt.bar(labels, scores)\n",
    "for b, s in zip(bars, scores):\n",
    "    plt.text(b.get_x()+b.get_width()/2, b.get_height()+0.005, f\"{s:.3f}\", ha='center', va='bottom', fontsize=10)\n",
    "plt.ylim(0,1.0); plt.ylabel(\"Accuracy\"); plt.title(\"Model Accuracy Comparison (hold-out test)\")\n",
    "plt.grid(axis='y', alpha=0.2); plt.tight_layout(); plt.savefig(OUT/\"model_accuracy_comparison_holdout.png\", dpi=300); plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_meta, labels=np.arange(NUM_CLASSES))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(NUM_CLASSES))\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "disp.plot(ax=ax, cmap='Blues', colorbar=True, values_format='d')\n",
    "plt.title(\"Confusion Matrix for the Meta-Learner\")\n",
    "plt.tight_layout(); plt.savefig(OUT/\"meta_confusion_matrix.png\", dpi=300); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 9) Cross-validation + multi-seed + CI + significance tests\n",
    "# -------------------------------------------------------\n",
    "def compute_ci(accs, alpha=0.05):\n",
    "    m  = float(np.mean(accs))\n",
    "    sd = float(np.std(accs, ddof=1)) if len(accs)>1 else 0.0\n",
    "    z  = 1.96\n",
    "    half = z*sd/math.sqrt(len(accs)) if len(accs)>1 else 0.0\n",
    "    return m, sd, (m-half, m+half)\n",
    "\n",
    "def to_rgb_array(x_uint8):\n",
    "    x = tf.convert_to_tensor(x_uint8)\n",
    "    x = tf.expand_dims(x, -1)\n",
    "    x = tf.image.resize(x, [IMG_SIZE, IMG_SIZE])\n",
    "    x = tf.image.grayscale_to_rgb(x)\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    return x.numpy()\n",
    "\n",
    "def build_penultimate(model, layer_name='gap'):\n",
    "    return tf.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "def predict_feats(fm, x, bs=64):\n",
    "    arr = []\n",
    "    for i in range(0, len(x), bs):\n",
    "        arr.append(fm.predict(x[i:i+bs], verbose=0))\n",
    "    return np.vstack(arr)\n",
    "\n",
    "def run_cv_multiseed(models_dict, x_uint8, y, seeds, kfolds):\n",
    "    results = {k: {'accs': [], 'preds': [], 'truths': []} for k in list(models_dict.keys()) + ['meta']}\n",
    "    for seed in seeds:\n",
    "        skf = StratifiedKFold(n_splits=kfolds, shuffle=True, random_state=seed)\n",
    "        x_rgb = to_rgb_array(x_uint8)\n",
    "        feat_models = {n: build_penultimate(m,'gap') for n,m in models_dict.items()}\n",
    "        for (i_tr, i_va) in skf.split(x_uint8, y):\n",
    "            x_tr, y_tr = x_rgb[i_tr], y[i_tr]\n",
    "            x_va, y_va = x_rgb[i_va], y[i_va]\n",
    "\n",
    "            # base\n",
    "            for name, model in models_dict.items():\n",
    "                yhat = np.argmax(model.predict(x_va, verbose=0), axis=1)\n",
    "                acc  = accuracy_score(y_va, yhat)\n",
    "                results[name]['accs'].append(acc)\n",
    "                results[name]['preds'].append(yhat)\n",
    "                results[name]['truths'].append(y_va)\n",
    "\n",
    "            # meta\n",
    "            F_tr = [predict_feats(feat_models[n], x_tr) for n in models_dict.keys()]\n",
    "            F_va = [predict_feats(feat_models[n], x_va) for n in models_dict.keys()]\n",
    "            X_tr = np.concatenate(F_tr, axis=1); X_va = np.concatenate(F_va, axis=1)\n",
    "            sc   = StandardScaler(); X_tr_s = sc.fit_transform(X_tr); X_va_s = sc.transform(X_va)\n",
    "            clf  = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n",
    "            clf.fit(X_tr_s, y_tr); yhat_m = clf.predict(X_va_s); acc_m = accuracy_score(y_va, yhat_m)\n",
    "            results['meta']['accs'].append(acc_m); results['meta']['preds'].append(yhat_m); results['meta']['truths'].append(y_va)\n",
    "\n",
    "    for name in results:\n",
    "        results[name]['preds']  = np.concatenate(results[name]['preds'])\n",
    "        results[name]['truths'] = np.concatenate(results[name]['truths'])\n",
    "    return results\n",
    "\n",
    "models_dict = {\"vgg16\": vgg16_model, \"vgg19\": vgg19_model, \"resnet50\": resnet50_model}\n",
    "cv_results  = run_cv_multiseed(models_dict, x_train, y_train, HP[\"cv_seeds\"], HP[\"cv_kfolds\"])\n",
    "\n",
    "# summary table\n",
    "rows = []\n",
    "for name in ['vgg16','vgg19','resnet50','meta']:\n",
    "    m, sd, (lo, hi) = compute_ci(cv_results[name]['accs'])\n",
    "    rows.append({\"Model\": name, \"MeanAcc\": m, \"Std\": sd, \"CI95_Low\": lo, \"CI95_High\": hi, \"N\": len(cv_results[name]['accs'])})\n",
    "df_cv = pd.DataFrame(rows)\n",
    "df_cv.to_csv(OUT/\"cv_summary.csv\", index=False)\n",
    "print(df_cv)\n",
    "\n",
    "# Significance tests\n",
    "from scipy.stats import wilcoxon\n",
    "def wilcoxon_vs_meta(cv_res, base_name):\n",
    "    a = np.array(cv_res['meta']['accs']); b = np.array(cv_res[base_name]['accs'])\n",
    "    stat, p = wilcoxon(a, b, zero_method='wilcox', correction=True, alternative='greater')\n",
    "    return float(stat), float(p)\n",
    "\n",
    "def mcnemar_test(truth, pred_a, pred_b):\n",
    "    n01 = np.sum((pred_a == truth) & (pred_b != truth))\n",
    "    n10 = np.sum((pred_a != truth) & (pred_b == truth))\n",
    "    n = n01 + n10\n",
    "    if n == 0: return 0.0, 1.0\n",
    "    chi2 = (abs(n01 - n10) - 1)**2 / n\n",
    "    try:\n",
    "        from scipy.stats import chi2 as chi2dist\n",
    "        p = float(chi2dist.sf(chi2, df=1))\n",
    "    except Exception:\n",
    "        p = np.exp(-0.5*chi2)\n",
    "    return float(chi2), float(p)\n",
    "\n",
    "sig_rows = []\n",
    "for base in ['vgg16','vgg19','resnet50']:\n",
    "    W, p = wilcoxon_vs_meta(cv_results, base)\n",
    "    sig_rows.append({\"Test\":\"Wilcoxon\", \"Comparison\": f\"meta > {base}\", \"W\": W, \"p\": p})\n",
    "\n",
    "best_base = max(['vgg16','vgg19','resnet50'], key=lambda k: np.mean(cv_results[k]['accs']))\n",
    "chi2, p = mcnemar_test(cv_results['meta']['truths'], cv_results['meta']['preds'], cv_results[best_base]['preds'])\n",
    "sig_rows.append({\"Test\":\"McNemar\", \"Comparison\": f\"meta vs {best_base}\", \"chi2\": chi2, \"p\": p})\n",
    "pd.DataFrame(sig_rows).to_csv(OUT/\"significance_tests.csv\", index=False)\n",
    "print(pd.DataFrame(sig_rows))\n",
    "\n",
    "# Boxplot per fold×seed\n",
    "plt.figure(figsize=(6.5,4))\n",
    "plt.boxplot([cv_results['vgg16']['accs'], cv_results['vgg19']['accs'], cv_results['resnet50']['accs'], cv_results['meta']['accs']],\n",
    "            labels=['VGG16','VGG19','ResNet50','Meta-LR'], showmeans=True)\n",
    "plt.ylabel(\"Accuracy\"); plt.title(\"Cross-validated Accuracies (per fold × seed)\"); plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout(); plt.savefig(OUT/\"cv_boxplot.png\", dpi=300); plt.show()\n",
    "\n",
    "# Bar chart of CV mean±std\n",
    "means = df_cv[\"MeanAcc\"].values; stds = df_cv[\"Std\"].values; labels = df_cv[\"Model\"].values\n",
    "plt.figure(figsize=(6.5,4))\n",
    "bars = plt.bar(labels, means, yerr=stds, capsize=4)\n",
    "for b, s in zip(bars, means):\n",
    "    plt.text(b.get_x()+b.get_width()/2, b.get_height()+0.005, f\"{s:.3f}\", ha='center', va='bottom', fontsize=10)\n",
    "plt.ylim(0,1.0); plt.ylabel(\"Accuracy\"); plt.title(\"Model Accuracy Comparison (CV mean ± std)\")\n",
    "plt.grid(axis='y', alpha=0.2); plt.tight_layout(); plt.savefig(OUT/\"model_accuracy_comparison_cv.png\", dpi=300); plt.show()\n",
    "\n",
    "print(\"\\\\nAll MNIST outputs saved under:\", OUT.resolve())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
